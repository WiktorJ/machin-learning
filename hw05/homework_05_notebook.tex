\section{Programming assignment 5}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{math}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{import} \PY{n}{load\PYZus{}breast\PYZus{}cancer}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{accuracy\PYZus{}score}\PY{p}{,} \PY{n}{f1\PYZus{}score}
\end{Verbatim}

    \hypertarget{your-task}{%
\subsection{Your task}\label{your-task}}

    In this notebook code skeleton for performing logistic regression with
gradient descent is given. Your task is to complete the functions where
required. You are only allowed to use built-in Python functions, as well
as any \texttt{numpy} functions. No other libraries / imports are
allowed.

    For numerical reasons, we actually minimize the following loss function

\[\mathcal{L}(\mathbf{w}) = \frac{1}{N} NLL(\mathbf{w}) +  \frac{1}{2}\lambda ||\mathbf{w}||^2_2\]

where \(NLL(\mathbf{w})\) is the negative log-likelihood function, as
defined in the lecture (Eq. 33)

    \hypertarget{exporting-the-results-to-pdf}{%
\subsection{Exporting the results to
PDF}\label{exporting-the-results-to-pdf}}

Once you complete the assignments, export the entire notebook as PDF and
attach it to your homework solutions. The best way of doing that is 1.
Run all the cells of the notebook. 2. Download the notebook in HTML
(click File \textgreater{} Download as \textgreater{} .html) 3. Convert
the HTML to PDF using e.g.~https://www.sejda.com/html-to-pdf or
\texttt{wkhtmltopdf} for Linux
(\href{https://www.cyberciti.biz/open-source/html-to-pdf-freeware-linux-osx-windows-software/}{tutorial})
4. Concatenate your solutions for other tasks with the output of Step 3.
On a Linux machine you can simply use \texttt{pdfunite}, there are
similar tools for other platforms too. You can only upload a single PDF
file to Moodle.

This way is preferred to using \texttt{nbconvert}, since
\texttt{nbconvert} clips lines that exceed page width and makes your
code harder to grade.

    \hypertarget{load-and-preprocess-the-data}{%
\subsection{Load and preprocess the
data}\label{load-and-preprocess-the-data}}

    In this assignment we will work with the UCI ML Breast Cancer Wisconsin
(Diagnostic) dataset https://goo.gl/U2Uwz2.

Features are computed from a digitized image of a fine needle aspirate
(FNA) of a breast mass. They describe characteristics of the cell nuclei
present in the image. There are 212 malignant examples and 357 benign
examples.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{load\PYZus{}breast\PYZus{}cancer}\PY{p}{(}\PY{n}{return\PYZus{}X\PYZus{}y}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Add a vector of ones to the data matrix to absorb the bias term}
        \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{[}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Set the random seed so that we have reproducible experiments}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{123}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Split into train and test}
        \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.3}
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{n}{test\PYZus{}size}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-1-implement-the-sigmoid-function}{%
\subsection{Task 1: Implement the sigmoid
function}\label{task-1-implement-the-sigmoid-function}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Applies the sigmoid function elementwise to the input data.}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Parameters}
        \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
        \PY{l+s+sd}{    t : array, arbitrary shape}
        \PY{l+s+sd}{        Input data.}
        \PY{l+s+sd}{        }
        \PY{l+s+sd}{    Returns}
        \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
        \PY{l+s+sd}{    t\PYZus{}sigmoid : array, arbitrary shape.}
        \PY{l+s+sd}{        Data after applying the sigmoid function.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{return} \PY{l+m+mi}{1} \PY{o}{/} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{t}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-2-implement-the-negative-log-likelihood}{%
\subsection{Task 2: Implement the negative log
likelihood}\label{task-2-implement-the-negative-log-likelihood}}

    As defined in Eq. 33

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{negative\PYZus{}log\PYZus{}likelihood}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Negative Log Likelihood of the Logistic Regression.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        (Augmented) feature matrix.}
         \PY{l+s+sd}{    y : array, shape [N]}
         \PY{l+s+sd}{        Classification targets.}
         \PY{l+s+sd}{    w : array, shape [D]}
         \PY{l+s+sd}{        Regression coefficients (w[0] is the bias term).}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    nll : float}
         \PY{l+s+sd}{        The negative log likelihood.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return}  \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{y}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)} 
                       \PY{o}{+} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \hypertarget{computing-the-loss-function-mathcallmathbfw-nothing-to-do-here}{%
\subsubsection{\texorpdfstring{Computing the loss function
\(\mathcal{L}(\mathbf{w})\) (nothing to do
here)}{Computing the loss function \textbackslash{}mathcal\{L\}(\textbackslash{}mathbf\{w\}) (nothing to do here)}}\label{computing-the-loss-function-mathcallmathbfw-nothing-to-do-here}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{k}{def} \PY{n+nf}{compute\PYZus{}loss}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{lmbda}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Negative Log Likelihood of the Logistic Regression.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        (Augmented) feature matrix.}
         \PY{l+s+sd}{    y : array, shape [N]}
         \PY{l+s+sd}{        Classification targets.}
         \PY{l+s+sd}{    w : array, shape [D]}
         \PY{l+s+sd}{        Regression coefficients (w[0] is the bias term).}
         \PY{l+s+sd}{    lmbda : float}
         \PY{l+s+sd}{        L2 regularization strength.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    loss : float}
         \PY{l+s+sd}{        Loss of the regularized logistic regression model.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{c+c1}{\PYZsh{} The bias term w[0] is not regularized by convention}
             \PY{k}{return} \PY{n}{negative\PYZus{}log\PYZus{}likelihood}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{)} \PY{o}{/} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)} \PYZbs{}
                    \PY{o}{+} \PY{n}{lmbda} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
\end{Verbatim}

    \hypertarget{task-3-implement-the-gradient-nabla_mathbfwmathcallmathbfw}{%
\subsection{\texorpdfstring{Task 3: Implement the gradient
\(\nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w})\)}{Task 3: Implement the gradient \textbackslash{}nabla\_\{\textbackslash{}mathbf\{w\}\}\textbackslash{}mathcal\{L\}(\textbackslash{}mathbf\{w\})}}\label{task-3-implement-the-gradient-nabla_mathbfwmathcallmathbfw}}

    Make sure that you compute the gradient of the loss function
\(\mathcal{L}(\mathbf{w})\) (not simply the NLL!)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}gradient}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{mini\PYZus{}batch\PYZus{}indices}\PY{p}{,} \PY{n}{lmbda}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Calculates the gradient (full or mini\PYZhy{}batch) of the negative log likelilhood w.r.t. w.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        (Augmented) feature matrix.}
         \PY{l+s+sd}{    y : array, shape [N]}
         \PY{l+s+sd}{        Classification targets.}
         \PY{l+s+sd}{    w : array, shape [D]}
         \PY{l+s+sd}{        Regression coefficients (w[0] is the bias term).}
         \PY{l+s+sd}{    mini\PYZus{}batch\PYZus{}indices: array, shape [mini\PYZus{}batch\PYZus{}size]}
         \PY{l+s+sd}{        The indices of the data points to be included in the (stochastic) calculation of the gradient.}
         \PY{l+s+sd}{        This includes the full batch gradient as well, if mini\PYZus{}batch\PYZus{}indices = np.arange(n\PYZus{}train).}
         \PY{l+s+sd}{    lmbda: float}
         \PY{l+s+sd}{        Regularization strentgh. lmbda = 0 means having no regularization.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    dw : array, shape [D]}
         \PY{l+s+sd}{        Gradient w.r.t. w.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{mini\PYZus{}batch\PYZus{}indices}\PY{p}{]}
             \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{mini\PYZus{}batch\PYZus{}indices}\PY{p}{]}
             \PY{k}{return} \PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)} \PY{o}{*} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{lmbda} \PY{o}{*} \PY{n}{w}
\end{Verbatim}

    \hypertarget{train-the-logistic-regression-model-nothing-to-do-here}{%
\subsubsection{Train the logistic regression model (nothing to do
here)}\label{train-the-logistic-regression-model-nothing-to-do-here}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k}{def} \PY{n+nf}{logistic\PYZus{}regression}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{num\PYZus{}steps}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{,} \PY{n}{mini\PYZus{}batch\PYZus{}size}\PY{p}{,} \PY{n}{lmbda}\PY{p}{,} \PY{n}{verbose}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Performs logistic regression with (stochastic) gradient descent.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        (Augmented) feature matrix.}
         \PY{l+s+sd}{    y : array, shape [N]}
         \PY{l+s+sd}{        Classification targets.}
         \PY{l+s+sd}{    num\PYZus{}steps : int}
         \PY{l+s+sd}{        Number of steps of gradient descent to perform.}
         \PY{l+s+sd}{    learning\PYZus{}rate: float}
         \PY{l+s+sd}{        The learning rate to use when updating the parameters w.}
         \PY{l+s+sd}{    mini\PYZus{}batch\PYZus{}size: int}
         \PY{l+s+sd}{        The number of examples in each mini\PYZhy{}batch.}
         \PY{l+s+sd}{        If mini\PYZus{}batch\PYZus{}size=n\PYZus{}train we perform full batch gradient descent. }
         \PY{l+s+sd}{    lmbda: float}
         \PY{l+s+sd}{        Regularization strentgh. lmbda = 0 means having no regularization.}
         \PY{l+s+sd}{    verbose : bool}
         \PY{l+s+sd}{        Whether to print the loss during optimization.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    w : array, shape [D]}
         \PY{l+s+sd}{        Optimal regression coefficients (w[0] is the bias term).}
         \PY{l+s+sd}{    trace: list}
         \PY{l+s+sd}{        Trace of the loss function after each step of gradient descent.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{n}{trace} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} saves the value of loss every 50 iterations to be able to plot it later}
             \PY{n}{n\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} number of training instances}
             
             \PY{n}{w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} initialize the parameters to zeros}
             
             \PY{c+c1}{\PYZsh{} run gradient descent for a given number of steps}
             \PY{k}{for} \PY{n}{step} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}steps}\PY{p}{)}\PY{p}{:}
                 \PY{n}{permuted\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{n\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{} shuffle the data}
                 
                 \PY{c+c1}{\PYZsh{} go over each mini\PYZhy{}batch and update the paramters}
                 \PY{c+c1}{\PYZsh{} if mini\PYZus{}batch\PYZus{}size = n\PYZus{}train we perform full batch GD and this loop runs only once}
                 \PY{k}{for} \PY{n}{idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{n\PYZus{}train}\PY{p}{,} \PY{n}{mini\PYZus{}batch\PYZus{}size}\PY{p}{)}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} get the random indices to be included in the mini batch}
                     \PY{n}{mini\PYZus{}batch\PYZus{}indices} \PY{o}{=} \PY{n}{permuted\PYZus{}idx}\PY{p}{[}\PY{n}{idx}\PY{p}{:}\PY{n}{idx}\PY{o}{+}\PY{n}{mini\PYZus{}batch\PYZus{}size}\PY{p}{]}
                     \PY{n}{gradient} \PY{o}{=} \PY{n}{get\PYZus{}gradient}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{mini\PYZus{}batch\PYZus{}indices}\PY{p}{,} \PY{n}{lmbda}\PY{p}{)}
         
                     \PY{c+c1}{\PYZsh{} update the parameters}
                     \PY{n}{w} \PY{o}{=} \PY{n}{w} \PY{o}{\PYZhy{}} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{gradient}
                 
                 \PY{c+c1}{\PYZsh{} calculate and save the current loss value every 50 iterations}
                 \PY{k}{if} \PY{n}{step} \PY{o}{\PYZpc{}} \PY{l+m+mi}{50} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                     \PY{n}{loss} \PY{o}{=} \PY{n}{compute\PYZus{}loss}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{lmbda}\PY{p}{)}
                     \PY{n}{trace}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{loss}\PY{p}{)}
                     \PY{c+c1}{\PYZsh{} print loss to monitor the progress}
                     \PY{k}{if} \PY{n}{verbose}\PY{p}{:}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step }\PY{l+s+si}{\PYZob{}0\PYZcb{}}\PY{l+s+s1}{, loss = }\PY{l+s+si}{\PYZob{}1:.4f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{step}\PY{p}{,} \PY{n}{loss}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{trace}
\end{Verbatim}

    \hypertarget{task-4-implement-the-function-to-obtain-the-predictions}{%
\subsection{Task 4: Implement the function to obtain the
predictions}\label{task-4-implement-the-function-to-obtain-the-predictions}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N\PYZus{}test, D]}
         \PY{l+s+sd}{        (Augmented) feature matrix.}
         \PY{l+s+sd}{    w : array, shape [D]}
         \PY{l+s+sd}{        Regression coefficients (w[0] is the bias term).}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    y\PYZus{}pred : array, shape [N\PYZus{}test]}
         \PY{l+s+sd}{        A binary array of predictions.}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{o}{.}\PY{n}{T}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{1}
\end{Verbatim}

    \hypertarget{full-batch-gradient-descent}{%
\subsubsection{Full batch gradient
descent}\label{full-batch-gradient-descent}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Change this to True if you want to see loss values over iterations.}
         \PY{n}{verbose} \PY{o}{=} \PY{k+kc}{False}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{n\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{w\PYZus{}full}\PY{p}{,} \PY{n}{trace\PYZus{}full} \PY{o}{=} \PY{n}{logistic\PYZus{}regression}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} 
                                                  \PY{n}{y\PYZus{}train}\PY{p}{,} 
                                                  \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{8000}\PY{p}{,} 
                                                  \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{,} 
                                                  \PY{n}{mini\PYZus{}batch\PYZus{}size}\PY{o}{=}\PY{n}{n\PYZus{}train}\PY{p}{,} 
                                                  \PY{n}{lmbda}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                                                  \PY{n}{verbose}\PY{o}{=}\PY{n}{verbose}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{n\PYZus{}train} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{w\PYZus{}minibatch}\PY{p}{,} \PY{n}{trace\PYZus{}minibatch} \PY{o}{=} \PY{n}{logistic\PYZus{}regression}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} 
                                                            \PY{n}{y\PYZus{}train}\PY{p}{,} 
                                                            \PY{n}{num\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{8000}\PY{p}{,} 
                                                            \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{,} 
                                                            \PY{n}{mini\PYZus{}batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{50}\PY{p}{,} 
                                                            \PY{n}{lmbda}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                                                            \PY{n}{verbose}\PY{o}{=}\PY{n}{verbose}\PY{p}{)}
\end{Verbatim}

    Our reference solution produces, but don't worry if yours is not exactly
the same.

\begin{verbatim}
Full batch: accuracy: 0.9240, f1_score: 0.9384
Mini-batch: accuracy: 0.9415, f1_score: 0.9533
\end{verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{y\PYZus{}pred\PYZus{}full} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{w\PYZus{}full}\PY{p}{)}
         \PY{n}{y\PYZus{}pred\PYZus{}minibatch} \PY{o}{=} \PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{w\PYZus{}minibatch}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Full batch: accuracy: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{, f1\PYZus{}score: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}full}\PY{p}{)}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}full}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mini\PYZhy{}batch: accuracy: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{, f1\PYZus{}score: }\PY{l+s+si}{\PYZob{}:.4f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}minibatch}\PY{p}{)}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred\PYZus{}minibatch}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Full batch: accuracy: 0.9240, f1\_score: 0.9384
Mini-batch: accuracy: 0.9415, f1\_score: 0.9528

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{trace\PYZus{}full}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Full batch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{trace\PYZus{}minibatch}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mini\PYZhy{}batch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iterations * 50}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss \PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mathcal}\PY{l+s+si}{\PYZob{}L\PYZcb{}}\PY{l+s+s1}{(}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mathbf}\PY{l+s+si}{\PYZob{}w\PYZcb{}}\PY{l+s+s1}{)\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
