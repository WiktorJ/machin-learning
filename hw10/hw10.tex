\documentclass{article} 
       
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{01\_homework\_knn}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    


    
    \usepackage{standalone}
    % \usepackage[utf8]{inputenc}

    % Default fixed font does not support bold face
    \DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
    \DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
    
    % Custom colors
    \usepackage{color}
    \definecolor{deepblue}{rgb}{0,0,0.5}
    \definecolor{deepred}{rgb}{0.6,0,0}
    \definecolor{deepgreen}{rgb}{0,0.5,0}
    
    \usepackage{listings}
    
    \lstdefinestyle{customasm}{
        xleftmargin=-60px,
        xrightmargin=-60px
    }   

    % Python style for highlighting
    \newcommand\pythonstyle{\lstset{
    language=Python,
    basicstyle=\small,
    otherkeywords={self},             % Add keywords here
    keywordstyle=\small\color{deepblue},
    emph={MyClass,__init__},          % Custom highlighting
    emphstyle=\small\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    frame=tb,                         % Any extra options here
    showstringspaces=false,
    breaklines=true,
    style=customasm             
    }}
    
    
    % Python environment
    \lstnewenvironment{python}[1][]
    {
    \pythonstyle
    \lstset{#1}
    }
    {}
    
    % Python for external files
    \newcommand\pythonexternal[2][]{{
    \pythonstyle
    \lstinputlisting[#1]{#2}}}
    
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator*{\argmin}{argmin}
    % Python for inline
    \newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
    % \usepackage{amsmath}
    \usepackage{hyperref}
    \usepackage{fancyvrb}
    \usepackage{enumitem}
    \usepackage{tikz}

\begin{document}
\title{Machine Learning homework 10 solution \\
        \large Dimensionality Reduction \& Matrix Factorization}
        \author{Wiktor Jurasz - M.Nr. 03709419}
\maketitle
\section{Problem 1}

For given model MLE estimates for $\mu_{ML}$:
\begin{equation}
    \mu_{ML} = \frac{1}{N}\sum_{i=1}^Nx_i = \bar{x}
\end{equation}
Thus 
\begin{equation}
    \bar{Ax} = \frac{1}{N}\sum_{i=1}^NAx_i = A\frac{1}{N}\sum_{i=1}^Nx_i = A\mu_{ML}
\end{equation}
\\\\\\
MLE estimate for $W_{ML}$
\begin{equation}
    W_{ML} = U(\Lambda - \sigma^2I)^{\frac{1}{2}}V
\end{equation}
The Covariance of $x$
\begin{equation}
    Cov(x) = S = U\Lambda U^T
\end{equation}
The Covariance of $Ax$
\begin{equation}
    Cov(x) = ASA^T = AU\Lambda U^TA^T = (AU)\Lambda (AU)^T
\end{equation}
We can see that in transformed space eigenvectors are also transformed 
by $A$, so from equation $3$ we have:
\begin{equation}
    Wtrans_{ML} = AU(\Lambda - \sigma^2I)^{\frac{1}{2}}V = AW_{ML}
\end{equation} 
\\\\\\
Model is given by:
\begin{equation}
    x_i = z_i + \mu + \epsilon_i
\end{equation}
By applying transformation $A$ we can see that error $\epsilon_i$ is also 
transformed by $A$
\begin{equation}
    Ax_i = Az_i + A\mu + A\epsilon_i
\end{equation}
As $\Phi$ is a covariance of the error we know (from lecture) that
\begin{equation}
    \Phi trans = A\Phi A^T \Rightarrow \Phi trans_{ML} = A\Phi_{ML} A^T 
\end{equation} 
\\\\\\
Finally by assuming orthogonality of $A$ and $\Phi = \sigma^2 I$
we can show that if noise $\epsilon_i$ of original space has distribution
$N(0, \sigma^2 I)$ then this property is preserved in transformed space.
\begin{equation}
    A\Phi A^T = A\sigma^2 IA^T = \sigma^2 AA^T = \sigma^2 I
\end{equation}
\section{Problem 2}
Projected data can be obtained by computing $M*V$. However we are only interested
in prediction for Leslie in concept space, so we can multiply just the new row 
by $V$. The result of this computation is $1x2$ vector $[1.74, 2.84]$ which gives
us information how much Leslie would like movies from both of concepts.

\section{Problem 3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}

    \hypertarget{exporting-the-results-to-pdf}{%
\subsection{Exporting the results to
PDF}\label{exporting-the-results-to-pdf}}

Once you complete the assignments, export the entire notebook as PDF and
attach it to your homework solutions. The best way of doing that is 1.
Run all the cells of the notebook. 2. Download the notebook in HTML
(click File \textgreater{} Download as \textgreater{} .html) 3. Convert
the HTML to PDF using e.g.~https://www.sejda.com/html-to-pdf or
\texttt{wkhtmltopdf} for Linux
(\href{https://www.cyberciti.biz/open-source/html-to-pdf-freeware-linux-osx-windows-software/}{tutorial})
4. Concatenate your solutions for other tasks with the output of Step 3.
On a Linux machine you can simply use \texttt{pdfunite}, there are
similar tools for other platforms too. You can only upload a single PDF
file to Moodle.

This way is preferred to using \texttt{nbconvert}, since
\texttt{nbconvert} clips lines that exceed page width and makes your
code harder to grade.

    \hypertarget{pca-task}{%
\subsection{PCA Task}\label{pca-task}}

    Given the data in the matrix X your tasks is to: * Calculate the
covariance matrix \(\Sigma\). * Calculate eigenvalues and eigenvectors
of \(\Sigma\). * Plot the original data \(X\) and the eigenvectors to a
single diagram. What do you observe? Which eigenvector corresponds to
the smallest eigenvalue? * Determine the smallest eigenvalue and remove
its corresponding eigenvector. The remaining eigenvector is the basis of
a new subspace. * Transform all vectors in X in this new subspace by
expressing all vectors in X in this new basis.

    \hypertarget{the-given-data-x}{%
\subsubsection{The given data X}\label{the-given-data-x}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                      \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                      \PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                      \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-1-calculate-the-covariance-matrix-sigma}{%
\subsubsection{\texorpdfstring{Task 1: Calculate the covariance matrix
\(\Sigma\)}{Task 1: Calculate the covariance matrix \textbackslash{}Sigma}}\label{task-1-calculate-the-covariance-matrix-sigma}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}covariance}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calculates the covariance matrix of the input data.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        Data matrix.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    Sigma : array, shape [D, D]}
         \PY{l+s+sd}{        Covariance matrix}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{rowvar}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-2-calculate-eigenvalues-and-eigenvectors-of-sigma.}{%
\subsubsection{\texorpdfstring{Task 2: Calculate eigenvalues and
eigenvectors of
\(\Sigma\).}{Task 2: Calculate eigenvalues and eigenvectors of \textbackslash{}Sigma.}}\label{task-2-calculate-eigenvalues-and-eigenvectors-of-sigma.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{get\PYZus{}eigen}\PY{p}{(}\PY{n}{S}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calculates the eigenvalues and eigenvectors of the input matrix.}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Parameters}
        \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
        \PY{l+s+sd}{    S : array, shape [D, D]}
        \PY{l+s+sd}{        Square symmetric positive definite matrix.}
        \PY{l+s+sd}{        }
        \PY{l+s+sd}{    Returns}
        \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
        \PY{l+s+sd}{    L : array, shape [D]}
        \PY{l+s+sd}{        Eigenvalues of S}
        \PY{l+s+sd}{    U : array, shape [D, D]}
        \PY{l+s+sd}{        Eigenvectors of S}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{eig}\PY{p}{(}\PY{n}{S}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-3-plot-the-original-data-x-and-the-eigenvectors-to-a-single-diagram.}{%
\subsubsection{Task 3: Plot the original data X and the eigenvectors to
a single
diagram.}\label{task-3-plot-the-original-data-x-and-the-eigenvectors-to-a-single-diagram.}}

Note that, in general if \(u_i\) is an eigenvector of the matrix \(M\)
with eigenvalue \(\lambda_i\) then \(\alpha \cdot u_i\) is also an
eigenvector of \(M\) with the same eigenvalue \(\lambda_i\), where
\(\alpha\) is an arbitrary scalar (including \(\alpha=-1\)).

Thus, the signs of the eigenvectors are arbitrary, and you can flip them
without changing the meaning of the result. Only their direction
matters. The particular result depends on the algorithm used to find
them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} plot the original data}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot the mean of the data}
         \PY{n}{mean\PYZus{}d1}\PY{p}{,} \PY{n}{mean\PYZus{}d2} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{mean\PYZus{}d1}\PY{p}{,} \PY{n}{mean\PYZus{}d2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} calculate the covariance matrix}
         \PY{n}{Sigma} \PY{o}{=} \PY{n}{get\PYZus{}covariance}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} calculate the eigenvector and eigenvalues of Sigma}
         \PY{n}{L}\PY{p}{,} \PY{n}{U} \PY{o}{=} \PY{n}{get\PYZus{}eigen}\PY{p}{(}\PY{n}{Sigma}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean\PYZus{}d1}\PY{p}{,} \PY{n}{mean\PYZus{}d2}\PY{p}{,} \PY{n}{U}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{U}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{arrow}\PY{p}{(}\PY{n}{mean\PYZus{}d1}\PY{p}{,} \PY{n}{mean\PYZus{}d2}\PY{p}{,} \PY{n}{U}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{U}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    What do you observe in the above plot? Which eigenvector corresponds to
the smallest eigenvalue?

Write your answer here:

Green vector corresponds to smaller eigenvalue as it points to the
direction of lower variance.

    \hypertarget{task-4-transform-the-data}{%
\subsubsection{Task 4: Transform the
data}\label{task-4-transform-the-data}}

    Determine the smallest eigenvalue and remove its corresponding
eigenvector. The remaining eigenvector is the basis of a new subspace.
Transform all vectors in X in this new subspace by expressing all
vectors in X in this new basis.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{k}{def} \PY{n+nf}{transform}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{U}\PY{p}{,} \PY{n}{L}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Transforms the data in the new subspace spanned by the eigenvector corresponding to the largest eigenvalue.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X : array, shape [N, D]}
         \PY{l+s+sd}{        Data matrix.}
         \PY{l+s+sd}{    L : array, shape [D]}
         \PY{l+s+sd}{        Eigenvalues of Sigma\PYZus{}X}
         \PY{l+s+sd}{    U : array, shape [D, D]}
         \PY{l+s+sd}{        Eigenvectors of Sigma\PYZus{}X}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    X\PYZus{}t : array, shape [N, 1]}
         \PY{l+s+sd}{        Transformed data}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{largest\PYZus{}index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{L}\PY{p}{)}
             \PY{n}{largest\PYZus{}eigenvector} \PY{o}{=} \PY{n}{U}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{largest\PYZus{}index}\PY{p}{]}
             \PY{k}{return} \PY{n}{X}\PY{n+nd}{@largest\PYZus{}eigenvector}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{X\PYZus{}t} \PY{o}{=} \PY{n}{transform}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{U}\PY{p}{,} \PY{n}{L}\PY{p}{)}
\end{Verbatim}

    \hypertarget{task-svd}{%
\subsection{Task SVD}\label{task-svd}}

    \hypertarget{task-5-given-the-matrix-m-find-its-svd-decomposition-m-u-cdot-sigma-cdot-v-and-reduce-it-to-one-dimension-using-the-approach-described-in-the-lecture.}{%
\subsubsection{\texorpdfstring{Task 5: Given the matrix \(M\) find its
SVD decomposition \(M= U \cdot \Sigma \cdot V\) and reduce it to one
dimension using the approach described in the
lecture.}{Task 5: Given the matrix M find its SVD decomposition M= U \textbackslash{}cdot \textbackslash{}Sigma \textbackslash{}cdot V and reduce it to one dimension using the approach described in the lecture.}}\label{task-5-given-the-matrix-m-find-its-svd-decomposition-m-u-cdot-sigma-cdot-v-and-reduce-it-to-one-dimension-using-the-approach-described-in-the-lecture.}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{M} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{k}{def} \PY{n+nf}{reduce\PYZus{}to\PYZus{}one\PYZus{}dimension}\PY{p}{(}\PY{n}{M}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Reduces the input matrix to one dimension using its SVD decomposition.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Parameters}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    M : array, shape [N, D]}
         \PY{l+s+sd}{        Input matrix.}
         \PY{l+s+sd}{        }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
         \PY{l+s+sd}{    M\PYZus{}t: array, shape [N, 1]}
         \PY{l+s+sd}{        Reduce matrix.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{U}\PY{p}{,}\PY{n}{S}\PY{p}{,}\PY{n}{V} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{svd}\PY{p}{(}\PY{n}{M}\PY{p}{,}\PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             \PY{k}{return} \PY{n}{M}\PY{o}{*}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{n}{M\PYZus{}t} \PY{o}{=} \PY{n}{reduce\PYZus{}to\PYZus{}one\PYZus{}dimension}\PY{p}{(}\PY{n}{M}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} 
\end{Verbatim}


    % Add a bibliography block to the postdoc
    
\section{Problem 4}    

\begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{time}
            \PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse} \PY{k}{as} \PY{n+nn}{sp}
            \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
            \PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{sparse}\PY{n+nn}{.}\PY{n+nn}{linalg} \PY{k}{import} \PY{n}{svds}
            \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}
            
            \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
            \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
    \end{Verbatim}
    
        \hypertarget{exporting-the-results-to-pdf}{%
    \subsection{Exporting the results to
    PDF}\label{exporting-the-results-to-pdf}}
    
    Once you complete the assignments, export the entire notebook as PDF and
    attach it to your homework solutions. The best way of doing that is 1.
    Run all the cells of the notebook. 2. Download the notebook in HTML
    (click File \textgreater{} Download as \textgreater{} .html) 3. Convert
    the HTML to PDF using e.g.~https://www.sejda.com/html-to-pdf or
    \texttt{wkhtmltopdf} for Linux
    (\href{https://www.cyberciti.biz/open-source/html-to-pdf-freeware-linux-osx-windows-software/}{tutorial})
    4. Concatenate your solutions for other tasks with the output of Step 3.
    On a Linux machine you can simply use \texttt{pdfunite}, there are
    similar tools for other platforms too. You can only upload a single PDF
    file to Moodle.
    
    This way is preferred to using \texttt{nbconvert}, since
    \texttt{nbconvert} clips lines that exceed page width and makes your
    code harder to grade.
    
        \hypertarget{restaurant-recommendation}{%
    \subsection{Restaurant recommendation}\label{restaurant-recommendation}}
    
    The goal of this task is to recommend restaurants to users based on the
    rating data in the Yelp dataset. For this, we try to predict the rating
    a user will give to a restaurant they have not yet rated based on a
    latent factor model.
    
    Specifically, the objective function (loss) we wanted to optimize is: \[
    \mathcal{L} = \min_{P, Q} \sum_{(i, x) \in W} (M_{ix} - \mathbf{q}_i^T\mathbf{p}_x)^2 + \lambda\sum_x{\left\lVert \mathbf{p}_x  \right\rVert}^2 + \lambda\sum_i {\left\lVert\mathbf{q}_i  \right\rVert}^2
    \]
    
    where \(W\) is the set of \((i, x)\) pairs for which the rating
    \(M_{ix}\) given by user \(i\) to restaurant \(x\) is known. Here we
    have also introduced two regularization terms to help us with
    overfitting where \(\lambda\) is hyper-parameter that control the
    strength of the regularization.
    
    \textbf{Hint 1}: Using the closed form solution for regression might
    lead to singular values. To avoid this issue perform the regression step
    with an existing package such as scikit-learn. It is advisable to use
    ridge regression to account for regularization.
    
    \textbf{Hint 2}: If you are using the scikit-learn package remember to
    set \texttt{fit\ intercept\ =\ False} to only learn the coeficients of
    the linear regression.
    
        \hypertarget{load-and-preprocess-the-data-nothing-to-do-here}{%
    \subsubsection{Load and Preprocess the Data (nothing to do
    here)}\label{load-and-preprocess-the-data-nothing-to-do-here}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}2}]:} \PY{n}{ratings} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ratings.npy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \end{Verbatim}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} We have triplets of (user, restaurant, rating).}
            \PY{n}{ratings}
    \end{Verbatim}
    
    \begin{Verbatim}[commandchars=\\\{\}]
    {\color{outcolor}Out[{\color{outcolor}3}]:} array([[101968,   1880,      1],
                   [101968,    284,      5],
                   [101968,   1378,      2],
                   {\ldots},
                   [ 72452,   2100,      4],
                   [ 72452,   2050,      5],
                   [ 74861,   3979,      5]])
    \end{Verbatim}
                
        Now we transform the data into a matrix of dimension {[}N, D{]}, where N
    is the number of users and D is the number of restaurants in the
    dataset. We store the data as a sparse matrix to avoid out-of-memory
    issues.
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}4}]:} \PY{n}{n\PYZus{}users} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{n\PYZus{}restaurants} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{M} \PY{o}{=} \PY{n}{sp}\PY{o}{.}\PY{n}{coo\PYZus{}matrix}\PY{p}{(}\PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{(}\PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ratings}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{n\PYZus{}users}\PY{p}{,} \PY{n}{n\PYZus{}restaurants}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{tocsr}\PY{p}{(}\PY{p}{)}
            \PY{n}{M}
    \end{Verbatim}
    
    \begin{Verbatim}[commandchars=\\\{\}]
    {\color{outcolor}Out[{\color{outcolor}4}]:} <337867x5899 sparse matrix of type '<class 'numpy.int64'>'
                with 929606 stored elements in Compressed Sparse Row format>
    \end{Verbatim}
                
        To avoid the cold start problem, in the preprocessing step, we
    recursively remove all users and restaurants with 10 or less ratings.
    
    Then, we randomly select 200 data points for the validation and test
    sets, respectively.
    
    After this, we subtract the mean rating for each users to account for
    this global effect.
    
    \textbf{Note}: Some entries might become zero in this process -- but
    these entries are different than the `unknown' zeros in the matrix. We
    store the indices for which we the rating data available in a separate
    variable.
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{l+s+sd}{    Recursively removes rows and columns from the input matrix which have less than min\PYZus{}entries nonzero entries.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    Parameters}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix      : sp.spmatrix, shape [N, D]}
            \PY{l+s+sd}{                  The input matrix to be preprocessed.}
            \PY{l+s+sd}{    min\PYZus{}entries : int}
            \PY{l+s+sd}{                  Minimum number of nonzero elements per row and column.}
            
            \PY{l+s+sd}{    Returns}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix      : sp.spmatrix, shape [N\PYZsq{}, D\PYZsq{}]}
            \PY{l+s+sd}{                  The pre\PYZhy{}processed matrix, where N\PYZsq{} \PYZlt{}= N and D\PYZsq{} \PYZlt{}= D}
            \PY{l+s+sd}{        }
            \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape before: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                
                \PY{n}{shape} \PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{k}{while} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape} \PY{o}{!=} \PY{n}{shape}\PY{p}{:}
                    \PY{n}{shape} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}
                    \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}
                    \PY{n}{row\PYZus{}ixs} \PY{o}{=} \PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}
                    \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix}\PY{p}{[}\PY{n}{row\PYZus{}ixs}\PY{p}{]}
                    \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}
                    \PY{n}{col\PYZus{}ixs} \PY{o}{=} \PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}
                    \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{col\PYZus{}ixs}\PY{p}{]}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape after: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{)}\PY{p}{)}
                \PY{n}{nnz} \PY{o}{=} \PY{n}{matrix}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}
                \PY{k}{assert} \PY{p}{(}\PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}
                \PY{k}{assert} \PY{p}{(}\PY{n}{nnz}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{A1} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}entries}\PY{p}{)}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{p}{)}
                \PY{k}{return} \PY{n}{matrix}
    \end{Verbatim}
    
        \hypertarget{task-1-implement-a-function-that-substracts-the-mean-user-rating-from-the-sparse-rating-matrix}{%
    \subsubsection{Task 1: Implement a function that substracts the mean
    user rating from the sparse rating
    matrix}\label{task-1-implement-a-function-that-substracts-the-mean-user-rating-from-the-sparse-rating-matrix}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{shift\PYZus{}user\PYZus{}mean}\PY{p}{(}\PY{n}{matrix}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{l+s+sd}{    Subtract the mean rating per user from the non\PYZhy{}zero elements in the input matrix.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    Parameters}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix : sp.spmatrix, shape [N, D]}
            \PY{l+s+sd}{             Input sparse matrix.}
            \PY{l+s+sd}{    Returns}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix : sp.spmatrix, shape [N, D]}
            \PY{l+s+sd}{             The modified input matrix.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    user\PYZus{}means : np.array, shape [N, 1]}
            \PY{l+s+sd}{                 The mean rating per user that can be used to recover the absolute ratings from the mean\PYZhy{}shifted ones.}
            
            \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                  
                \PY{c+c1}{\PYZsh{} YOUR CODE HERE}
                \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{matrix} \PY{o}{=} \PY{n}{matrix} \PY{o}{\PYZhy{}} \PY{n}{user\PYZus{}means}
                \PY{k}{assert} \PY{n}{np}\PY{o}{.}\PY{n}{all}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{isclose}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
                \PY{k}{return} \PY{n}{matrix}\PY{p}{,} \PY{n}{user\PYZus{}means}
    \end{Verbatim}
    
        \hypertarget{split-the-data-into-a-train-validation-and-test-set-nothing-to-do-here}{%
    \subsubsection{Split the data into a train, validation and test set
    (nothing to do
    here)}\label{split-the-data-into-a-train-validation-and-test-set-nothing-to-do-here}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}7}]:} \PY{k}{def} \PY{n+nf}{split\PYZus{}data}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{n\PYZus{}validation}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}\PY{p}{:}
                \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{l+s+sd}{    Extract validation and test entries from the input matrix. }
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    Parameters}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix          : sp.spmatrix, shape [N, D]}
            \PY{l+s+sd}{                      The input data matrix.}
            \PY{l+s+sd}{    n\PYZus{}validation    : int}
            \PY{l+s+sd}{                      The number of validation entries to extract.}
            \PY{l+s+sd}{    n\PYZus{}test          : int}
            \PY{l+s+sd}{                      The number of test entries to extract.}
            
            \PY{l+s+sd}{    Returns}
            \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
            \PY{l+s+sd}{    matrix\PYZus{}split    : sp.spmatrix, shape [N, D]}
            \PY{l+s+sd}{                      A copy of the input matrix in which the validation and test entries have been set to zero.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    val\PYZus{}idx         : tuple, shape [2, n\PYZus{}validation]}
            \PY{l+s+sd}{                      The indices of the validation entries.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    test\PYZus{}idx        : tuple, shape [2, n\PYZus{}test]}
            \PY{l+s+sd}{                      The indices of the test entries.}
            \PY{l+s+sd}{    }
            \PY{l+s+sd}{    val\PYZus{}values      : np.array, shape [n\PYZus{}validation, ]}
            \PY{l+s+sd}{                      The values of the input matrix at the validation indices.}
            \PY{l+s+sd}{                      }
            \PY{l+s+sd}{    test\PYZus{}values     : np.array, shape [n\PYZus{}test, ]}
            \PY{l+s+sd}{                      The values of the input matrix at the test indices.}
            
            \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                
                \PY{n}{matrix\PYZus{}cp} \PY{o}{=} \PY{n}{matrix}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                \PY{n}{non\PYZus{}zero\PYZus{}idx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{matrix\PYZus{}cp}\PY{p}{)}
                \PY{n}{ixs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{permutation}\PY{p}{(}\PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{)}
                \PY{n}{val\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{p}{:}\PY{n}{n\PYZus{}validation}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                \PY{n}{test\PYZus{}idx} \PY{o}{=} \PY{n+nb}{tuple}\PY{p}{(}\PY{n}{ixs}\PY{p}{[}\PY{n}{n\PYZus{}validation}\PY{p}{:}\PY{n}{n\PYZus{}validation} \PY{o}{+} \PY{n}{n\PYZus{}test}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}
                
                \PY{n}{val\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
                \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
                
                \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{matrix\PYZus{}cp}\PY{p}{[}\PY{n}{test\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
                \PY{n}{matrix\PYZus{}cp}\PY{o}{.}\PY{n}{eliminate\PYZus{}zeros}\PY{p}{(}\PY{p}{)}
            
                \PY{k}{return} \PY{n}{matrix\PYZus{}cp}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values}
    \end{Verbatim}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}8}]:} \PY{n}{M} \PY{o}{=} \PY{n}{cold\PYZus{}start\PYZus{}preprocessing}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}
    \end{Verbatim}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    Shape before: (337867, 5899)
    Shape after: (3529, 2072)
    
        \end{Verbatim}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}9}]:} \PY{n}{n\PYZus{}validation} \PY{o}{=} \PY{l+m+mi}{200}
            \PY{n}{n\PYZus{}test} \PY{o}{=} \PY{l+m+mi}{200}
            \PY{c+c1}{\PYZsh{} Split data}
            \PY{n}{M\PYZus{}train}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{test\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,} \PY{n}{test\PYZus{}values} \PY{o}{=} \PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{n\PYZus{}validation}\PY{p}{,} \PY{n}{n\PYZus{}test}\PY{p}{)}
    \end{Verbatim}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Remove user means.}
             \PY{n}{nonzero\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argwhere}\PY{p}{(}\PY{n}{M\PYZus{}train}\PY{p}{)}
             \PY{n}{M\PYZus{}shifted}\PY{p}{,} \PY{n}{user\PYZus{}means} \PY{o}{=} \PY{n}{shift\PYZus{}user\PYZus{}mean}\PY{p}{(}\PY{n}{M\PYZus{}train}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Apply the same shift to the validation and test data.}
             \PY{n}{val\PYZus{}values\PYZus{}shifted} \PY{o}{=} \PY{n}{val\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{val\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
             \PY{n}{test\PYZus{}values\PYZus{}shifted} \PY{o}{=} \PY{n}{test\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{user\PYZus{}means}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{test\PYZus{}idx}\PY{p}{)}\PY{o}{.}\PY{n}{T}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{A1}
    \end{Verbatim}
    
        \hypertarget{compute-the-loss-function-nothing-to-do-here}{%
    \subsubsection{Compute the loss function (nothing to do
    here)}\label{compute-the-loss-function-nothing-to-do-here}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}11}]:} \PY{k}{def} \PY{n+nf}{loss}\PY{p}{(}\PY{n}{values}\PY{p}{,} \PY{n}{ixs}\PY{p}{,} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{reg\PYZus{}lambda}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{l+s+sd}{    Compute the loss of the latent factor model (at indices ixs).}
             \PY{l+s+sd}{    Parameters}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    values : np.array, shape [n\PYZus{}ixs,]}
             \PY{l+s+sd}{        The array with the ground\PYZhy{}truth values.}
             \PY{l+s+sd}{    ixs : tuple, shape [2, n\PYZus{}ixs]}
             \PY{l+s+sd}{        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).}
             \PY{l+s+sd}{    Q : np.array, shape [N, k]}
             \PY{l+s+sd}{        The matrix Q of a latent factor model.}
             \PY{l+s+sd}{    P : np.array, shape [k, D]}
             \PY{l+s+sd}{        The matrix P of a latent factor model.}
             \PY{l+s+sd}{    reg\PYZus{}lambda : float}
             \PY{l+s+sd}{        The regularization strength}
             \PY{l+s+sd}{          }
             \PY{l+s+sd}{    Returns}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    loss : float}
             \PY{l+s+sd}{           The loss of the latent factor model.}
             
             \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n}{mean\PYZus{}sse\PYZus{}loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{values} \PY{o}{\PYZhy{}} \PY{n}{Q}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{P}\PY{p}{)}\PY{p}{[}\PY{n}{ixs}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
                 \PY{n}{regularization\PYZus{}loss} \PY{o}{=}  \PY{n}{reg\PYZus{}lambda} \PY{o}{*} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{P}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{Q}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
                 
                 \PY{k}{return} \PY{n}{mean\PYZus{}sse\PYZus{}loss} \PY{o}{+} \PY{n}{regularization\PYZus{}loss}
    \end{Verbatim}
    
        \hypertarget{alternating-optimization}{%
    \subsection{Alternating optimization}\label{alternating-optimization}}
    
    In the first step, we will approach the problem via alternating
    optimization, as learned in the lecture. That is, during each iteration
    you first update \(Q\) while having \(P\) fixed and then vice versa.
    
        \hypertarget{task-2-implement-a-function-that-initializes-the-latent-factors-q-and-p}{%
    \subsubsection{\texorpdfstring{Task 2: Implement a function that
    initializes the latent factors \(Q\) and
    \(P\)}{Task 2: Implement a function that initializes the latent factors Q and P}}\label{task-2-implement-a-function-that-initializes-the-latent-factors-q-and-p}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{l+s+sd}{    Initialize the matrices Q and P for a latent factor model.}
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    Parameters}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    matrix : sp.spmatrix, shape [N, D]}
             \PY{l+s+sd}{             The matrix to be factorized.}
             \PY{l+s+sd}{    k      : int}
             \PY{l+s+sd}{             The number of latent dimensions.}
             \PY{l+s+sd}{    init   : str in [\PYZsq{}svd\PYZsq{}, \PYZsq{}random\PYZsq{}], default: \PYZsq{}random\PYZsq{}}
             \PY{l+s+sd}{             The initialization strategy. \PYZsq{}svd\PYZsq{} means that we use SVD to initialize P and Q, \PYZsq{}random\PYZsq{} means we initialize}
             \PY{l+s+sd}{             the entries in P and Q randomly in the interval [0, 1).}
             
             \PY{l+s+sd}{    Returns}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    Q : np.array, shape [N, k]}
             \PY{l+s+sd}{        The initialized matrix Q of a latent factor model.}
             
             \PY{l+s+sd}{    P : np.array, shape [k, D]}
             \PY{l+s+sd}{        The initialized matrix P of a latent factor model.}
             \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
                 \PY{k}{if} \PY{n}{init}\PY{o}{==}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                     \PY{n}{Q} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{k}\PY{p}{)}\PY{p}{)}
                     \PY{n}{P} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{random}\PY{p}{(}\PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{n}{Q}\PY{p}{,}\PY{n}{\PYZus{}}\PY{p}{,}\PY{n}{P} \PY{o}{=} \PY{n}{svds}\PY{p}{(}\PY{n}{matrix}\PY{p}{,} \PY{n}{full\PYZus{}matrices}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                     
                 \PY{k}{assert} \PY{n}{Q}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{k}\PY{p}{)}
                 \PY{k}{assert} \PY{n}{P}\PY{o}{.}\PY{n}{shape} \PY{o}{==} \PY{p}{(}\PY{n}{k}\PY{p}{,} \PY{n}{matrix}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 \PY{k}{return} \PY{n}{Q}\PY{p}{,} \PY{n}{P}
    \end{Verbatim}
    
        \hypertarget{task-3-implement-the-alternating-optimization-approach}{%
    \subsubsection{Task 3: Implement the alternating optimization
    approach}\label{task-3-implement-the-alternating-optimization-approach}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}38}]:} \PY{k}{def} \PY{n+nf}{latent\PYZus{}factor\PYZus{}alternating\PYZus{}optimization}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{non\PYZus{}zero\PYZus{}idx}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{p}{,} \PY{n}{val\PYZus{}values}\PY{p}{,}
                                                        \PY{n}{reg\PYZus{}lambda}\PY{p}{,} \PY{n}{max\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                        \PY{n}{log\PYZus{}every}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{eval\PYZus{}every}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{l+s+sd}{    Perform matrix factorization using alternating optimization. Training is done via patience,}
             \PY{l+s+sd}{    i.e. we stop training after we observe no improvement on the validation loss for a certain}
             \PY{l+s+sd}{    amount of training steps. We then return the best values for Q and P oberved during training.}
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    Parameters}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    M                 : sp.spmatrix, shape [N, D]}
             \PY{l+s+sd}{                        The input matrix to be factorized.}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    non\PYZus{}zero\PYZus{}idx      : np.array, shape [nnz, 2]}
             \PY{l+s+sd}{                        The indices of the non\PYZhy{}zero entries of the un\PYZhy{}shifted matrix to be factorized. }
             \PY{l+s+sd}{                        nnz refers to the number of non\PYZhy{}zero entries. Note that this may be different}
             \PY{l+s+sd}{                        from the number of non\PYZhy{}zero entries in the input matrix M, e.g. in the case}
             \PY{l+s+sd}{                        that all ratings by a user have the same value.}
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    k                 : int}
             \PY{l+s+sd}{                        The latent factor dimension.}
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    val\PYZus{}idx           : tuple, shape [2, n\PYZus{}validation]}
             \PY{l+s+sd}{                        Tuple of the validation set indices.}
             \PY{l+s+sd}{                        n\PYZus{}validation refers to the size of the validation set.}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    val\PYZus{}values        : np.array, shape [n\PYZus{}validation, ]}
             \PY{l+s+sd}{                        The values in the validation set.}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    reg\PYZus{}lambda        : float}
             \PY{l+s+sd}{                        The regularization strength.}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    max\PYZus{}steps         : int, optional, default: 100}
             \PY{l+s+sd}{                        Maximum number of training steps. Note that we will stop early if we observe}
             \PY{l+s+sd}{                        no improvement on the validation error for a specified number of steps}
             \PY{l+s+sd}{                        (see \PYZdq{}patience\PYZdq{} for details).}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    init              : str in [\PYZsq{}random\PYZsq{}, \PYZsq{}svd\PYZsq{}], default \PYZsq{}random\PYZsq{}}
             \PY{l+s+sd}{                        The initialization strategy for P and Q. See function initialize\PYZus{}Q\PYZus{}P for details.}
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    log\PYZus{}every         : int, optional, default: 1}
             \PY{l+s+sd}{                        Log the training status every X iterations.}
             \PY{l+s+sd}{                    }
             \PY{l+s+sd}{    patience          : int, optional, default: 5}
             \PY{l+s+sd}{                        Stop training after we observe no improvement of the validation loss for X evaluation}
             \PY{l+s+sd}{                        iterations (see eval\PYZus{}every for details). After we stop training, we restore the best }
             \PY{l+s+sd}{                        observed values for Q and P (based on the validation loss) and return them.}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    eval\PYZus{}every        : int, optional, default: 1}
             \PY{l+s+sd}{                        Evaluate the training and validation loss every X steps. If we observe no improvement}
             \PY{l+s+sd}{                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.}
             
             \PY{l+s+sd}{    Returns}
             \PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
             \PY{l+s+sd}{    best\PYZus{}Q            : np.array, shape [N, k]}
             \PY{l+s+sd}{                        Best value for Q (based on validation loss) observed during training}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    best\PYZus{}P            : np.array, shape [k, D]}
             \PY{l+s+sd}{                        Best value for P (based on validation loss) observed during training}
             \PY{l+s+sd}{                      }
             \PY{l+s+sd}{    validation\PYZus{}losses : list of floats}
             \PY{l+s+sd}{                        Validation loss for every evaluation iteration, can be used for plotting the validation}
             \PY{l+s+sd}{                        loss over time.}
             \PY{l+s+sd}{                        }
             \PY{l+s+sd}{    train\PYZus{}losses      : list of floats}
             \PY{l+s+sd}{                        Training loss for every evaluation iteration, can be used for plotting the training}
             \PY{l+s+sd}{                        loss over time.                     }
             \PY{l+s+sd}{    }
             \PY{l+s+sd}{    converged\PYZus{}after   : int}
             \PY{l+s+sd}{                        it \PYZhy{} patience*eval\PYZus{}every, where it is the iteration in which patience hits 0,}
             \PY{l+s+sd}{                        or \PYZhy{}1 if we hit max\PYZus{}steps before converging. }
             
             \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
                 \PY{n}{validation\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{train\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{reg} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{reg\PYZus{}lambda}\PY{p}{,} \PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
                 \PY{n}{Q}\PY{p}{,}\PY{n}{P} \PY{o}{=} \PY{n}{initialize\PYZus{}Q\PYZus{}P}\PY{p}{(}\PY{n}{M}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{init}\PY{p}{)}
                 \PY{n}{t} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{current\PYZus{}p} \PY{o}{=} \PY{l+m+mi}{0}
                 \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
                 \PY{n}{best\PYZus{}Q}\PY{p}{,} \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{Q}\PY{p}{,} \PY{n}{P}
                 \PY{n}{best\PYZus{}valid\PYZus{}lost} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{w\PYZus{}indexes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{n}{h\PYZus{}indexes} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                 \PY{k}{for} \PY{n}{el} \PY{o+ow}{in} \PY{n}{nonzero\PYZus{}indices}\PY{p}{:}
                     \PY{n}{w\PYZus{}indexes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{el}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                     \PY{n}{h\PYZus{}indexes}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{el}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 \PY{n}{m\PYZus{}idx} \PY{o}{=} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{w\PYZus{}indexes}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{h\PYZus{}indexes}\PY{p}{)}\PY{p}{)}
                 \PY{k}{for} \PY{n}{step} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}steps}\PY{p}{)}\PY{p}{:}
                     \PY{k}{if} \PY{n}{current\PYZus{}p} \PY{o}{==} \PY{n}{patience}\PY{p}{:}
                         \PY{n}{converged\PYZus{}after} \PY{o}{=} \PY{n}{step} \PY{o}{\PYZhy{}} \PY{n}{patience}\PY{o}{*}\PY{n}{eval\PYZus{}every}
                         \PY{k}{break}\PY{p}{;}
                     \PY{n}{P} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{=}\PY{n}{Q}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{M}\PY{p}{)}\PY{o}{.}\PY{n}{coef\PYZus{}}
                     \PY{n}{Q} \PY{o}{=} \PY{n}{reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{=}\PY{n}{P}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n}{M}\PY{o}{.}\PY{n}{T}\PY{p}{)}\PY{o}{.}\PY{n}{coef\PYZus{}}
                     \PY{k}{if} \PY{n}{step} \PY{o}{\PYZpc{}} \PY{n}{eval\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n}{pred} \PY{o}{=} \PY{n}{Q}\PY{n+nd}{@P}\PY{o}{.}\PY{n}{T}
                         \PY{n}{train\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{(}\PY{p}{(}\PY{n}{M}\PY{p}{[}\PY{n}{m\PYZus{}idx}\PY{p}{]}\PY{o}{.}\PY{n}{T} \PY{o}{\PYZhy{}} \PY{n}{pred}\PY{p}{[}\PY{n}{m\PYZus{}idx}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                         \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{val\PYZus{}values} \PY{o}{\PYZhy{}} \PY{n}{pred}\PY{p}{[}\PY{n}{val\PYZus{}idx}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                         \PY{n}{validation\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}
                         \PY{k}{if} \PY{n}{val\PYZus{}loss} \PY{o}{\PYZlt{}} \PY{n}{best\PYZus{}valid\PYZus{}lost}\PY{p}{:}
                             \PY{n}{best\PYZus{}valid\PYZus{}lost} \PY{o}{=} \PY{n}{val\PYZus{}loss}
                             \PY{n}{best\PYZus{}P} \PY{o}{=} \PY{n}{P}
                             \PY{n}{best\PYZus{}Q} \PY{o}{=} \PY{n}{Q}
                         \PY{k}{if} \PY{n}{validation\PYZus{}losses}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{validation\PYZus{}losses}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{val\PYZus{}loss}\PY{p}{:}
                             \PY{n}{current\PYZus{}p} \PY{o}{=} \PY{n}{current\PYZus{}p} \PY{o}{+} \PY{l+m+mi}{1}
                         \PY{k}{else}\PY{p}{:}
                             \PY{n}{current\PYZus{}pent\PYZus{}p} \PY{o}{=} \PY{l+m+mi}{0}
                     \PY{k}{if} \PY{n}{step} \PY{o}{\PYZpc{}} \PY{n}{log\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Iteration }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, training loss: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{, validation loss: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                               \PY{n}{step}\PY{p}{,} \PY{n}{train\PYZus{}losses}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{train\PYZus{}losses}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                               \PY{n}{validation\PYZus{}losses}\PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{validation\PYZus{}losses}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 \PY{k}{return} \PY{n}{best\PYZus{}Q}\PY{p}{,} \PY{n}{best\PYZus{}P}\PY{p}{,} \PY{n}{validation\PYZus{}losses}\PY{p}{,} \PY{n}{train\PYZus{}losses}\PY{p}{,} \PY{n}{converged\PYZus{}after}
    \end{Verbatim}
    
        \hypertarget{train-the-latent-factor-nothing-to-do-here}{%
    \subsubsection{Train the latent factor (nothing to do
    here)}\label{train-the-latent-factor-nothing-to-do-here}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor} }]:} \PY{n}{Q}\PY{p}{,} \PY{n}{P}\PY{p}{,} \PY{n}{val\PYZus{}loss}\PY{p}{,} \PY{n}{train\PYZus{}loss}\PY{p}{,} \PY{n}{converged} \PY{o}{=} \PYZbs{}
            \PY{n}{latent\PYZus{}factor\PYZus{}alternating\PYZus{}optimization}\PY{p}{(}\PY{n}{M\PYZus{}shifted}\PY{p}{,} \PY{n}{nonzero\PYZus{}indices}\PY{p}{,} 
                                                    \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{val\PYZus{}idx}\PY{o}{=}\PY{n}{val\PYZus{}idx}\PY{p}{,}
                                                    \PY{n}{val\PYZus{}values}\PY{o}{=}\PY{n}{val\PYZus{}values\PYZus{}shifted}\PY{p}{,} 
                                                    \PY{n}{reg\PYZus{}lambda}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}4}\PY{p}{,} \PY{n}{init}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                                    \PY{n}{max\PYZus{}steps}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{,} \PY{n}{patience}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
    \end{Verbatim}
    
        \hypertarget{plot-the-validation-and-training-losses-over-for-each-iteration-nothing-to-do-here}{%
    \subsubsection{Plot the validation and training losses over for each
    iteration (nothing to do
    here)}\label{plot-the-validation-and-training-losses-over-for-each-iteration-nothing-to-do-here}}
    
        \begin{Verbatim}[commandchars=\\\{\}]
    {\color{incolor}In [{\color{incolor}15}]:} \PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}
             \PY{n}{fig}\PY{o}{.}\PY{n}{suptitle}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alternating optimization, k=100}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{)}
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             
             
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{p}{]}\PY{p}{)}
             \PY{n}{ax}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Training iteration}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
    \end{Verbatim}
    
        \begin{center}
        \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_0.png}
        \end{center}
        { \hspace*{\fill} \\}
        
    
        % Add a bibliography block to the postdoc
        
        
        I didn't manage to finish \textit{latent\_factor\_alternating\_optimization} function implementation.      
    
\section{Problem 5}
With identity activation function autoencoder is not able to approximate non-linear
function. Thus in order for $K$ dimensional reduction to give $0$ reconstruction error
all the data points in $D$ would have to exist in lower dimension $K$ perfectly, that is without
any noise. Even if on point is slightly "off" the hyperplane this information is lost during encoding.
Usually it is the case that collected data contain some noise.
\\\\
Perfect reconstruction is only possible if $D-K$ dimensions are linear combinations
of the other $K$ dimensions.
\end{document}
